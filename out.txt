INFO 07-01 17:24:56 [__init__.py:244] Automatically detected platform rocm.
INFO 07-01 17:25:08 [config.py:1988] Disabled the custom all-reduce kernel because it is not supported on current platform.
INFO 07-01 17:25:08 [api_server.py:1287] vLLM API server version 0.9.2.dev46+gb6efafd9e
INFO 07-01 17:25:08 [config.py:1988] Disabled the custom all-reduce kernel because it is not supported on current platform.
INFO 07-01 17:25:08 [cli_args.py:309] non-default args: {'model': '/lustre/orion/stf218/world-shared/palashmr/HF_backup/nemotron4m', 'trust_remote_code': True, 'dtype': 'bfloat16', 'max_model_len': 131072, 'served_model_name': ['nemotron'], 'tensor_parallel_size': 8, 'gpu_memory_utilization': 0.85}
WARNING 07-01 17:25:08 [utils.py:2442] Found ulimit of 16384 and failed to automatically increase with error current limit exceeds maximum limit. This can cause fd limit errors like `OSError: [Errno 24] Too many open files`. Consider increasing with ulimit -n
INFO 07-01 17:25:30 [config.py:831] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 07-01 17:25:30 [config.py:3276] Downcasting torch.float32 to torch.bfloat16.
INFO 07-01 17:25:30 [config.py:1954] Defaulting to use mp for distributed inference
INFO 07-01 17:25:30 [config.py:1988] Disabled the custom all-reduce kernel because it is not supported on current platform.
INFO 07-01 17:25:30 [config.py:2203] Chunked prefill is enabled with max_num_batched_tokens=2048.
INFO 07-01 17:25:38 [__init__.py:244] Automatically detected platform rocm.
INFO 07-01 17:25:42 [core.py:455] Waiting for init message from front-end.
INFO 07-01 17:25:42 [core.py:70] Initializing a V1 LLM engine (v0.9.2.dev46+gb6efafd9e) with config: model='/lustre/orion/stf218/world-shared/palashmr/HF_backup/nemotron4m', speculative_config=None, tokenizer='/lustre/orion/stf218/world-shared/palashmr/HF_backup/nemotron4m', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=nemotron, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
WARNING 07-01 17:25:42 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 07-01 17:25:43 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3, 4, 5, 6, 7], buffer_handle=(8, 16777216, 10, 'psm_1d7300e1'), local_subscribe_addr='ipc:///tmp/331483a1-a074-476d-8729-1fdaef503f99', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 07-01 17:25:49 [__init__.py:244] Automatically detected platform rocm.
INFO 07-01 17:25:49 [__init__.py:244] Automatically detected platform rocm.
INFO 07-01 17:25:49 [__init__.py:244] Automatically detected platform rocm.
INFO 07-01 17:25:49 [__init__.py:244] Automatically detected platform rocm.
INFO 07-01 17:25:49 [__init__.py:244] Automatically detected platform rocm.
INFO 07-01 17:25:49 [__init__.py:244] Automatically detected platform rocm.
INFO 07-01 17:25:49 [__init__.py:244] Automatically detected platform rocm.
INFO 07-01 17:25:49 [__init__.py:244] Automatically detected platform rocm.
WARNING 07-01 17:25:54 [utils.py:2746] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f9979f66a80>
[1;36m(VllmWorker rank=3 pid=228210)[0;0m INFO 07-01 17:25:54 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_50e236a3'), local_subscribe_addr='ipc:///tmp/bcfba220-006e-4df4-81dc-0a07cbeba47d', remote_subscribe_addr=None, remote_addr_ipv6=False)
WARNING 07-01 17:25:54 [utils.py:2746] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7feca7e54110>
[1;36m(VllmWorker rank=6 pid=228213)[0;0m INFO 07-01 17:25:54 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_25ff38a6'), local_subscribe_addr='ipc:///tmp/7099a705-ffad-405b-9e44-c2970af50e56', remote_subscribe_addr=None, remote_addr_ipv6=False)
WARNING 07-01 17:25:54 [utils.py:2746] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fadafbe7a70>
[1;36m(VllmWorker rank=4 pid=228211)[0;0m INFO 07-01 17:25:54 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_5925f9fe'), local_subscribe_addr='ipc:///tmp/071507f0-13f3-4cfc-8c29-eafc25928b48', remote_subscribe_addr=None, remote_addr_ipv6=False)
WARNING 07-01 17:25:54 [utils.py:2746] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f1e45bb0140>
[1;36m(VllmWorker rank=0 pid=228207)[0;0m INFO 07-01 17:25:54 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_a5686be2'), local_subscribe_addr='ipc:///tmp/fe5d7712-5174-47a8-9e86-306425fff379', remote_subscribe_addr=None, remote_addr_ipv6=False)
WARNING 07-01 17:25:54 [utils.py:2746] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fb09deedfd0>
[1;36m(VllmWorker rank=7 pid=228214)[0;0m INFO 07-01 17:25:54 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_2b5c9337'), local_subscribe_addr='ipc:///tmp/0ffa9ef3-999e-4eda-964a-b694b7d334af', remote_subscribe_addr=None, remote_addr_ipv6=False)
WARNING 07-01 17:25:54 [utils.py:2746] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f1174dd7e30>
[1;36m(VllmWorker rank=5 pid=228212)[0;0m INFO 07-01 17:25:54 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_1cb2b9a9'), local_subscribe_addr='ipc:///tmp/dcae92b1-b316-45e0-bd7a-7d7b7ef0dbe9', remote_subscribe_addr=None, remote_addr_ipv6=False)
WARNING 07-01 17:25:54 [utils.py:2746] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f6190ea3680>
[1;36m(VllmWorker rank=1 pid=228208)[0;0m INFO 07-01 17:25:54 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_54456a8f'), local_subscribe_addr='ipc:///tmp/74c6c59a-ad8e-4ac2-8164-84dee808f111', remote_subscribe_addr=None, remote_addr_ipv6=False)
WARNING 07-01 17:25:54 [utils.py:2746] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f86dd5c40e0>
[1;36m(VllmWorker rank=2 pid=228209)[0;0m INFO 07-01 17:25:54 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_0de7af44'), local_subscribe_addr='ipc:///tmp/cffa134b-d362-41b2-9df2-a17193083bc3', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=2 pid=228209)[0;0m ERROR 07-01 17:26:02 [multiproc_executor.py:492] WorkerProc failed to start.
[1;36m(VllmWorker rank=2 pid=228209)[0;0m ERROR 07-01 17:26:02 [multiproc_executor.py:492] Traceback (most recent call last):
[1;36m(VllmWorker rank=2 pid=228209)[0;0m ERROR 07-01 17:26:02 [multiproc_executor.py:492]   File "/lustre/orion/stf218/proj-shared/brave/vllm/vllm/v1/executor/multiproc_executor.py", line 466, in worker_main
[1;36m(VllmWorker rank=2 pid=228209)[0;0m ERROR 07-01 17:26:02 [multiproc_executor.py:492]     worker = WorkerProc(*args, **kwargs)
[1;36m(VllmWorker rank=2 pid=228209)[0;0m ERROR 07-01 17:26:02 [multiproc_executor.py:492]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=2 pid=228209)[0;0m ERROR 07-01 17:26:02 [multiproc_executor.py:492]   File "/lustre/orion/stf218/proj-shared/brave/vllm/vllm/v1/executor/multiproc_executor.py", line 362, in __init__
[1;36m(VllmWorker rank=2 pid=228209)[0;0m ERROR 07-01 17:26:02 [multiproc_executor.py:492]     self.worker.init_device()
[1;36m(VllmWorker rank=2 pid=228209)[0;0m ERROR 07-01 17:26:02 [multiproc_executor.py:492]   File "/lustre/orion/stf218/proj-shared/brave/vllm/vllm/worker/worker_base.py", line 606, in init_device
[1;36m(VllmWorker rank=2 pid=228209)[0;0m ERROR 07-01 17:26:02 [multiproc_executor.py:492]     self.worker.init_device()  # type: ignore
[1;36m(VllmWorker rank=2 pid=228209)[0;0m ERROR 07-01 17:26:02 [multiproc_executor.py:492]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=2 pid=228209)[0;0m ERROR 07-01 17:26:02 [multiproc_executor.py:492]   File "/lustre/orion/stf218/proj-shared/brave/vllm/vllm/v1/worker/gpu_worker.py", line 128, in init_device
[1;36m(VllmWorker rank=2 pid=228209)[0;0m ERROR 07-01 17:26:02 [multiproc_executor.py:492]     torch.cuda.set_device(self.device)
[1;36m(VllmWorker rank=2 pid=228209)[0;0m ERROR 07-01 17:26:02 [multiproc_executor.py:492]   File "/ccs/home/palashmr/packages/miniconda/pyt_env/vllm-rocm/lib/python3.12/site-packages/torch/cuda/__init__.py", line 541, in set_device
[1;36m(VllmWorker rank=2 pid=228209)[0;0m ERROR 07-01 17:26:02 [multiproc_executor.py:492]     torch._C._cuda_setDevice(device)
[1;36m(VllmWorker rank=2 pid=228209)[0;0m ERROR 07-01 17:26:02 [multiproc_executor.py:492] torch.AcceleratorError: HIP error: invalid device ordinal
[1;36m(VllmWorker rank=2 pid=228209)[0;0m ERROR 07-01 17:26:02 [multiproc_executor.py:492] HIP kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[1;36m(VllmWorker rank=2 pid=228209)[0;0m ERROR 07-01 17:26:02 [multiproc_executor.py:492] For debugging consider passing AMD_SERIALIZE_KERNEL=3
[1;36m(VllmWorker rank=2 pid=228209)[0;0m ERROR 07-01 17:26:02 [multiproc_executor.py:492] Compile with `TORCH_USE_HIP_DSA` to enable device-side assertions.
[1;36m(VllmWorker rank=2 pid=228209)[0;0m ERROR 07-01 17:26:02 [multiproc_executor.py:492] 
ERROR 07-01 17:26:05 [core.py:515] EngineCore failed to start.
ERROR 07-01 17:26:05 [core.py:515] Traceback (most recent call last):
ERROR 07-01 17:26:05 [core.py:515]   File "/lustre/orion/stf218/proj-shared/brave/vllm/vllm/v1/engine/core.py", line 506, in run_engine_core
ERROR 07-01 17:26:05 [core.py:515]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 07-01 17:26:05 [core.py:515]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 07-01 17:26:05 [core.py:515]   File "/lustre/orion/stf218/proj-shared/brave/vllm/vllm/v1/engine/core.py", line 390, in __init__
ERROR 07-01 17:26:05 [core.py:515]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 07-01 17:26:05 [core.py:515]   File "/lustre/orion/stf218/proj-shared/brave/vllm/vllm/v1/engine/core.py", line 76, in __init__
ERROR 07-01 17:26:05 [core.py:515]     self.model_executor = executor_class(vllm_config)
ERROR 07-01 17:26:05 [core.py:515]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 07-01 17:26:05 [core.py:515]   File "/lustre/orion/stf218/proj-shared/brave/vllm/vllm/executor/executor_base.py", line 53, in __init__
ERROR 07-01 17:26:05 [core.py:515]     self._init_executor()
ERROR 07-01 17:26:05 [core.py:515]   File "/lustre/orion/stf218/proj-shared/brave/vllm/vllm/v1/executor/multiproc_executor.py", line 98, in _init_executor
ERROR 07-01 17:26:05 [core.py:515]     self.workers = WorkerProc.wait_for_ready(unready_workers)
ERROR 07-01 17:26:05 [core.py:515]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 07-01 17:26:05 [core.py:515]   File "/lustre/orion/stf218/proj-shared/brave/vllm/vllm/v1/executor/multiproc_executor.py", line 427, in wait_for_ready
ERROR 07-01 17:26:05 [core.py:515]     raise e from None
ERROR 07-01 17:26:05 [core.py:515] Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
